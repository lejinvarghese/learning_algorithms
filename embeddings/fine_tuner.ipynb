{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune text embeddings\n",
    "[basic](https://huggingface.co/blog/how-to-train-sentence-transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers datasets -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import InputExample, SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sentence_transformers import losses\n",
    "\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "- In all cases, negatives are implicitly created, whether we provide them or not. The negatives are created by the model itself, and are the other documents in the batch. This is why we don't need to provide negatives in the dataset. When we provide the explicit negatives, the model will use them instead of the implicit negatives.\n",
    "\n",
    "| dataset_structure           | examples                                                                          | loss                                                                                                | application                                                           |\n",
    "|-----------------------------|-----------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------|\n",
    "| <query, document, label>    | snli                                                                              | ContrastiveLoss; SoftmaxLoss; CosineSimilarityLoss                                                  | natural language inference (NLI) {entailment, netural, contradiciton} |\n",
    "| <query, document>           | embedding-data/flickr30k_captions_quintets; embedding-data/coco_captions_quintets; embedding-data/sentence-compression | MultipleNegativesRankingLoss; MegaBatchMarginLoss                                                   | natural language inference (NLI) {entailment}                         |\n",
    "| <query, class>              | trec; yahoo_answers_topics                                                        | BatchHardTripletLoss; BatchAllTripletLoss; BatchHardSoftMarginTripletLoss; BatchSemiHardTripletLoss |                                                                       |\n",
    "| <query, document, negative> | embedding-data/QQP_triplets                                                       | TripletLoss;                                                                                        |                                                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: <query, document, negative>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding-data/QQP_triplets dataset has 101762 examples.\n",
      "Each example is a <class 'dict'> with a <class 'dict'> as value.\n",
      "Examples look like this: {'set': {'query': 'Why do you use an iPhone?', 'pos': ['Why do people buy the iPhone?'], 'neg': [\"Why shouldn't I buy an iPhone?\", 'Why is iPhone so expensive?', 'Why are iPhones so expensive?', 'Why iphone are so costly?', 'Why are iPhones costly?', 'Is the iPhone really more expensive? Why or why not?', 'Why people are madly buying iPhone 4 in India, given that it is a more than 3-year-old hardware?', 'Why should I not buy the iPhone 5?', 'Why should I not buy an iPhone 7?', 'Why do some people prefer iPhones to Androids?', 'What are the reasons why people buy Samsung phones?', 'Why are iPhone users so loyal to the brand?', 'Why is the iPhone 6 so expensive?', 'Are iPhones seriously worth the price?', 'Are Apple iPhones worth the price?', 'Why is the iPhone 6s so expensive?', 'Is the iPhone really worth its price?', 'Is iPhone really worth the money or is it just a hype?', 'Is the iPhone SE worth buying?', 'What iPhone apps are worth buying?', \"Why can't the people who manufacture iPhones quit their jobs and create their own iPhones to sell?\", 'Is iphone worth it?', 'Why do iPhone lovers hate Android and vice-versa?', 'What do you hate about iPhone?', 'Why do people think it is unnecessary to spend 50k INR on an iPhone?', \"Why doesn't Apple buy Samsung?\", 'What do people think about the iPhone SE?', 'Why is the iPhone 5c so expensive?', 'Do people prefer iPhone or Android?', 'Why is it that an iPhone 4 is cheaper in the USA while it is way costlier in India? Is it only because it is \"Unlocked\"?']}}\n",
      "Positives: 1\n",
      "Negatives: 30\n"
     ]
    }
   ],
   "source": [
    "dataset_id = \"embedding-data/QQP_triplets\"\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "sample = dataset[\"train\"][-1]\n",
    "\n",
    "print(f\"The {dataset_id} dataset has {dataset['train'].num_rows} examples.\")\n",
    "print(f\"Each example is a {type(sample)} with a {type(sample['set'])} as value.\")\n",
    "\n",
    "print(f\"Examples look like this: {sample}\")\n",
    "print(f\"Positives: {len(sample.get('set').get('pos'))}\")\n",
    "print(f\"Negatives: {len(sample.get('set').get('neg'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: <query, document>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding-data/sentence-compression dataset has 180000 examples.\n",
      "Each example is a <class 'dict'> with a <class 'list'> as value.\n",
      "Examples look like this: {'set': ['Two of the most annoying forms of musical expression might all too soon converge to the sound of shrieking, sophomoric orchestral crescendos and controversy.', 'Two most annoying forms of musical expression converge...']}\n"
     ]
    }
   ],
   "source": [
    "dataset_id = \"embedding-data/sentence-compression\"\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "\n",
    "sample = dataset[\"train\"][-1]\n",
    "\n",
    "print(f\"The {dataset_id} dataset has {dataset['train'].num_rows} examples.\")\n",
    "print(f\"Each example is a {type(sample)} with a {type(sample['set'])} as value.\")\n",
    "\n",
    "print(f\"Examples look like this: {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 2045 examples.\n",
      "Examples look like this: {'set': ['ACC Commissioner John Swofford shakes the hand of Notre Dame president Rev. John I. Jenkins after Notre Dame announced it would join the ACC. The Fighting Irish will maintain an independent football team.', 'Notre Dame joins the ACC']}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def contains_topic(dataset, topic=\"sport|football|soccer\"):\n",
    "    return any(\n",
    "        re.search(rf\"\\b({topic})\\b\", text, re.IGNORECASE) for text in dataset[\"set\"]\n",
    "    )\n",
    "\n",
    "\n",
    "topic_dataset = dataset[\"train\"].filter(contains_topic)\n",
    "print(f\"The dataset has {topic_dataset.num_rows} examples.\")\n",
    "print(f\"Examples look like this: {topic_dataset[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "train_data = topic_dataset[\"set\"]\n",
    "n_examples = topic_dataset.num_rows\n",
    "max_samples = 10_000\n",
    "\n",
    "for i in range(n_examples)[:max_samples]:\n",
    "    example = train_data[i]\n",
    "    train_examples.append(InputExample(texts=[example[0], example[1]]))\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"embedding-data/distilroberta-base-sentence-transformer\"\n",
    "raw_model = SentenceTransformer(model_id)\n",
    "model = SentenceTransformer(model_id)\n",
    "\n",
    "assert model is not raw_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 32/32 [02:12<00:00,  4.15s/it]\n",
      "Iteration: 100%|██████████| 32/32 [01:13<00:00,  2.30s/it]\n",
      "Iteration: 100%|██████████| 32/32 [01:51<00:00,  3.49s/it]\n",
      "Iteration: 100%|██████████| 32/32 [01:48<00:00,  3.39s/it]\n",
      "Iteration: 100%|██████████| 32/32 [01:53<00:00,  3.54s/it]\n",
      "Iteration: 100%|██████████| 32/32 [02:31<00:00,  4.72s/it]\n",
      "Iteration: 100%|██████████| 32/32 [02:33<00:00,  4.80s/it]\n",
      "Iteration: 100%|██████████| 32/32 [02:07<00:00,  3.98s/it]\n",
      "Iteration: 100%|██████████| 32/32 [01:44<00:00,  3.27s/it]\n",
      "Iteration: 100%|██████████| 32/32 [01:46<00:00,  3.33s/it]\n",
      "Epoch: 100%|██████████| 10/10 [19:43<00:00, 118.31s/it]\n"
     ]
    }
   ],
   "source": [
    "loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "num_epochs = 10\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, loss)],\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    query = \"I love playing soccer by the beach, it's fun! I play the midfield position, and love passing the ball. My favorite team is Liverpool and try to play like them.\"\n",
    "    documents = [\n",
    "        \"I love Liverpool!\",\n",
    "        \"Midfielders love passing the ball\",\n",
    "        \"I love playing volleyball while I'm at the beach\",\n",
    "        \"I enjoy watching sports\",\n",
    "        \"I enjoy cooking dinner for my family\",\n",
    "        \"I enjoy dancing, and live music on a night out at the bar\",\n",
    "        \"I love dogs!\",\n",
    "    ]\n",
    "\n",
    "    query_embedding = model.encode([query])\n",
    "    document_embeddings = model.encode(documents)\n",
    "\n",
    "    similarities = util.cos_sim(query_embedding, document_embeddings)\n",
    "\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    results = sorted(\n",
    "        zip(documents, similarities[0].tolist()), key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "    return pd.DataFrame(results, columns=[\"document\", \"similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding similarity from the raw model:\n",
      "Query: I love playing soccer by the beach, it's fun! I play the midfield position, and love passing the ball. My favorite team is Liverpool and try to play like them.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love playing volleyball while I'm at the beach</td>\n",
       "      <td>0.850552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Midfielders love passing the ball</td>\n",
       "      <td>0.795377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love Liverpool!</td>\n",
       "      <td>0.795222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I enjoy dancing, and live music on a night out...</td>\n",
       "      <td>0.793633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I enjoy watching sports</td>\n",
       "      <td>0.756239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I love dogs!</td>\n",
       "      <td>0.752807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I enjoy cooking dinner for my family</td>\n",
       "      <td>0.717749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  similarity\n",
       "0   I love playing volleyball while I'm at the beach    0.850552\n",
       "1                  Midfielders love passing the ball    0.795377\n",
       "2                                  I love Liverpool!    0.795222\n",
       "3  I enjoy dancing, and live music on a night out...    0.793633\n",
       "4                            I enjoy watching sports    0.756239\n",
       "5                                       I love dogs!    0.752807\n",
       "6               I enjoy cooking dinner for my family    0.717749"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding similarity from the fine-tuned model:\n",
      "Query: I love playing soccer by the beach, it's fun! I play the midfield position, and love passing the ball. My favorite team is Liverpool and try to play like them.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love playing volleyball while I'm at the beach</td>\n",
       "      <td>0.695838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love Liverpool!</td>\n",
       "      <td>0.622702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Midfielders love passing the ball</td>\n",
       "      <td>0.571108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I enjoy watching sports</td>\n",
       "      <td>0.494715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I enjoy dancing, and live music on a night out...</td>\n",
       "      <td>0.442614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I love dogs!</td>\n",
       "      <td>0.408467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I enjoy cooking dinner for my family</td>\n",
       "      <td>0.267176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  similarity\n",
       "0   I love playing volleyball while I'm at the beach    0.695838\n",
       "1                                  I love Liverpool!    0.622702\n",
       "2                  Midfielders love passing the ball    0.571108\n",
       "3                            I enjoy watching sports    0.494715\n",
       "4  I enjoy dancing, and live music on a night out...    0.442614\n",
       "5                                       I love dogs!    0.408467\n",
       "6               I enjoy cooking dinner for my family    0.267176"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Embedding similarity from the raw model:\")\n",
    "display(predict(raw_model))\n",
    "\n",
    "print(\"Embedding similarity from the fine-tuned model:\")\n",
    "display(predict(model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explainin",
   "language": "python",
   "name": "explain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
