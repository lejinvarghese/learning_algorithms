{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import animation, rc\n",
    "from qbstyles import mpl_style\n",
    "\n",
    "plt.style.use(\"seaborn-white\")\n",
    "plt.rcParams.update({\"pdf.fonttype\": \"truetype\"})\n",
    "\n",
    "# mpl_style(dark=False)\n",
    "np.set_printoptions(suppress=True, precision = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Reinforcement Learning for Supply Chain and Price Optimization**\n",
    "\n",
    "This is the first part of the tutorial focused on price optimization. We define an environment with a complex price-demand function and implement DQN that jointly optimizes the price schedule for several time steps ahead.\n",
    "\n",
    "- [Detailed Description](https://blog.griddynamics.com/deep-reinforcement-learning-for-supply-chain-and-price-optimization/)\n",
    "- [Paper](https://ewrl.files.wordpress.com/2018/09/ewrl_14_2018_paper_44.pdf)\n",
    "\n",
    "# **Environment: Assymetric Price Response and Hi-Lo Pricing**\n",
    "\n",
    "Traditional price optimization assumes a simple known parametric price-response function. Optimization under such an assumption is relatively straightforward, even in case of inventory constraints or other factors. Many basic formulations can be solved analytically, and many others can be solved numerically. The challenge is typically in estimating the price-response function, not in the optimization.\n",
    "\n",
    "In some cases, however, optimization also becomes a challenge. This section introduces an example of a price-demand function with temporal dependencies. The optimal price schedule for such a function has a **sawtooth** shape.\n",
    "\n",
    "The revenue optimization problem can be defined as below:\n",
    "\n",
    "\\begin{aligned} \n",
    "\\max \\ \\ & \\sum_t \\sum_j p_j \\cdot q(t, j) \\cdot x_{tj} \\\\ \n",
    "\\text{subject to} \\ \\ & \\sum_j x_{tj} = 1, \\quad \\text{for all } t \\\\ \n",
    "&\\sum_t \\sum_j q(t, j) \\cdot x_{tj} = c \\\\ \n",
    "&x_{tj} \\in {0,1} \n",
    "\\end{aligned}\n",
    "\n",
    "where:\n",
    "- $t$ - time intervals\n",
    "- $j$ - price levels\n",
    "- $p_{j}$ - price at level $j$\n",
    "- $q(t, j)$ - demand at time $t$ at level $j$\n",
    "- $c$ - total stock\n",
    "- $x_{tj}$ - binary variable equal to 1 if price level $p_{j}$ is assigned at time $t$, 0 otherwise\n",
    "\n",
    "there are two constraints:\n",
    "- each time interval has only one allowed price\n",
    "- all demands sum up to the available stock\n",
    "\n",
    "This is an integer programming problem that can be solved using conventional optimization libraries.\n",
    "\n",
    "# **Temporal Dependency of Demand and Price**\n",
    "\n",
    "In the real world, demand depends not only on the current absolute price level but can also be impacted by the magnitude of **recent price changes** and even **past prices**. Price decrease can create a temporary **demand splash**, while the price increase can create a **sticker shock** and a temporary drop in demand. The impact of price changes can also be **asymmetric**.\n",
    "\n",
    "\n",
    "\\begin{aligned} \n",
    "q(p_t, p_{t-1}) &= q_0 - k\\cdot p_t - a\\cdot s( (p_t - p_{t-1})^+) + b\\cdot s( (p_t - p_{t-1})^-) \\\\ \n",
    "\\end{aligned}\n",
    "\n",
    "where:\n",
    "- $(p_t - p_{t-1})^+$ = $p_t - p_{t-1}$ if $p_t$ > $p_{t-1}$, else $0$\n",
    "- $(p_t - p_{t-1})^-$ = $p_t - p_{t-1}$ if $p_t$ < $p_{t-1}$, else $0$\n",
    "- $p_{t}$ - price at time $t$\n",
    "- $q_{0}$, $k$ - corresponds to a linear demand model with intercept $q_{0}$, price slope $k$\n",
    "- $a$, $b$ - sensitivity to positive and negative price changes\n",
    "- $s$ - shock function to specify a non-linear dependency between price change and demand (for illustration purposes, we can assume that $s(x)$ = $\\sqrt{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price Levels: (99,):\n",
      "[  5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90\n",
      "  95 100 105 110 115 120 125 130 135 140 145 150 155 160 165 170 175 180\n",
      " 185 190 195 200 205 210 215 220 225 230 235 240 245 250 255 260 265 270\n",
      " 275 280 285 290 295 300 305 310 315 320 325 330 335 340 345 350 355 360\n",
      " 365 370 375 380 385 390 395 400 405 410 415 420 425 430 435 440 445 450\n",
      " 455 460 465 470 475 480 485 490 495]\n",
      "\n",
      "Price Changes: (25,):\n",
      "[0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95 1.   1.05 1.1  1.15\n",
      " 1.2  1.25 1.3  1.35 1.4  1.45 1.5  1.55 1.6  1.65 1.7 ]\n"
     ]
    }
   ],
   "source": [
    "# environment simulator\n",
    "\n",
    "T = 20                                     # time steps in the price schedule\n",
    "price_max = 500                  # maximum valid price\n",
    "price_step = 5                       # minimum price change possible\n",
    "price_inc_perc = 0.5            # maximum percentage price increase\n",
    "price_dec_perc = 0.75        # minimum percentage price increase\n",
    "q_0 = 5000                           # intercept in the demand function q_t\n",
    "k = 20                                    # slope in the demand function q_t\n",
    "cost = 80                               # item cost\n",
    "a_q = 300                             # coefficient of price increase\n",
    "b_q = 100                             # coefficient of price decrease\n",
    "\n",
    "def p_rectified(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def n_rectified(x):\n",
    "    return max(0, -x)\n",
    "\n",
    "def shock(x):\n",
    "    return np.sqrt(x)\n",
    "\n",
    "# demand at time step t for current price p_t and previous price p_t_1\n",
    "def q_t(p_t, p_t_1, q_0, k, a, b):\n",
    "    return p_rectified(q_0 - k * p_t - a*shock(p_rectified(p_t - p_t_1)) + b*shock(n_rectified(p_t - p_t_1)))\n",
    "\n",
    "# margin at time step t\n",
    "def margin_t(p_t, p_t_1, q_0, k, a, b, cost):\n",
    "    return  (p_t - cost) * q_t(p_t, p_t_1, q_0, k, a, b)\n",
    "\n",
    "# total margin for the price vector p over the len(p) time steps\n",
    "def margin_total(p, q_0, k, a, b, cost):\n",
    "    return margin_t(p[0], p[0], q_0, k, 0, 0, cost) + sum(map(lambda t: margin_t(p[t], p[t-1], q_0, k, a, b, cost), range(len(p))))\n",
    "\n",
    "# partial bindings for better reabability \n",
    "def margin_t_resp(p_t, p_t_1):\n",
    "    return np.round(margin_t(p_t, p_t_1, q_0, k, a_q, b_q, cost))\n",
    "\n",
    "def margin_total_resp(p):\n",
    "    return np.round(margin_total(p, q_0, k, a_q, b_q, cost))\n",
    "\n",
    "\n",
    "price_levels = np.arange(price_step, price_max, price_step)\n",
    "price_changes = np.arange(1 - price_inc_perc, 1 + price_dec_perc, 0.05)\n",
    "demand_matrix = np.zeros((len(price_levels), len(price_changes)))\n",
    "revenue_matrix = np.zeros((len(price_levels), len(price_changes)))\n",
    "margin_matrix = np.zeros((len(price_levels), len(price_changes)))\n",
    "\n",
    "for i, p_l in enumerate(price_levels):\n",
    "    for j, p_c in enumerate(price_changes):\n",
    "        demand_matrix[i, j] = q_t(p_t = p_l, p_t_1 = p_l * p_c, q_0 = q_0, k = k, a = a_q, b = b_q)\n",
    "        revenue_matrix[i, j] = p_l * demand_matrix[i, j]\n",
    "        margin_matrix[i, j] = margin_t_resp(p_t = p_l, p_t_1 = p_l * p_c)\n",
    "        \n",
    "print(f\"Price Levels: {np.shape(price_levels)}:\\n{price_levels}\\n\\nPrice Changes: {np.shape(np.int_(np.round((1-price_changes)*100)))}:\\n{price_changes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot price-demand functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure(1600x500)\n",
      "Figure(1600x500)\n",
      "Figure(1600x500)\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "for i, p_c in enumerate(price_changes):\n",
    "    if math.isclose(p_c, 1.0):\n",
    "        color = 'black'\n",
    "        linewidth = 4.0\n",
    "    else:\n",
    "        p_norm = (p_c - min(price_changes))/ (max(price_changes) - min(price_changes))\n",
    "        color = (p_norm, 0.4, 1 - p_norm)\n",
    "        linewidth = 1.0\n",
    "    plt.plot(price_levels, demand_matrix[:, i], c=color, linewidth = linewidth)\n",
    "plt.xlabel(\"Price ($)\")\n",
    "plt.ylabel(\"Demand\")\n",
    "plt.title(\"Demand vs Price\")\n",
    "plt.legend(np.int_(np.round((1-price_changes)*100)), loc='right', title=\"Price change (%)\", fancybox=False, framealpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "for i, p_c in enumerate(price_changes):\n",
    "    if math.isclose(p_c, 1.0):\n",
    "        color = 'black'\n",
    "        linewidth = 4.0\n",
    "    else:\n",
    "        p_norm = (p_c - min(price_changes))/ (max(price_changes) - min(price_changes))\n",
    "        color = (p_norm, 0.4, 1 - p_norm)\n",
    "        linewidth = 1.0\n",
    "    plt.plot(price_levels, revenue_matrix[:, i], c=color, linewidth = linewidth)\n",
    "plt.xlabel(\"Price ($)\")\n",
    "plt.ylabel(\"Revenue \")\n",
    "plt.title(\"Revenue vs Price\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "for i, p_c in enumerate(price_changes):\n",
    "    if math.isclose(p_c, 1.0):\n",
    "        color = 'black'\n",
    "        linewidth = 4.0\n",
    "    else:\n",
    "        p_norm = (p_c - min(price_changes))/ (max(price_changes) - min(price_changes))\n",
    "        color = (p_norm, 0.4, 1 - p_norm)\n",
    "        linewidth = 1.0\n",
    "    plt.plot(price_levels, margin_matrix[:, i], c=color, linewidth = linewidth)\n",
    "plt.xlabel(\"Price ($)\")\n",
    "plt.ylabel(\"Margin\")\n",
    "plt.title(\"Margin vs Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Optimum Price and Margin for Static Price Level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal price is 165, achieved margin is 3034500.0\n"
     ]
    }
   ],
   "source": [
    "margins = np.array([margin_total_resp(np.repeat(price, T)) for price in price_levels])\n",
    "p_opt_static = price_levels[np.argmax(margins)]\n",
    "print(f'Optimal price is {p_opt_static}, achieved margin is {margins[np.argmax(margins)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Optimum Price and Margin for Dynamic Price Levels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sequential Greedy Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165\n",
      " 165 165]\n",
      "[165 495 215 180 170 165 495 215 180 170 165 495 215 180 170 165 495 215\n",
      " 180 170]\n",
      "Achieved profit is 3566604.0\n",
      "Figure(1600x500)\n"
     ]
    }
   ],
   "source": [
    "def find_opt_price_at_t(price_baseline, price_levels, t):\n",
    "    _price_actuals = np.tile(price_baseline, (len(price_levels), 1))\n",
    "    _price_actuals[:, t] = price_levels\n",
    "#     print(_price_actuals, np.shape (_price_actuals))\n",
    "    margins = np.array([margin_total_resp(prices) for prices in _price_actuals])\n",
    "#     print(np.shape(margins))\n",
    "    return price_levels[np.argmax(margins)]\n",
    "\n",
    "p_opt_dyn_sched = np.repeat(p_opt_static, T)\n",
    "print(p_opt_dyn_sched)\n",
    "for t in range(T):\n",
    "    price_t = find_opt_price_at_t(p_opt_dyn_sched, price_levels, t)\n",
    "    p_opt_dyn_sched[t] = price_t\n",
    "\n",
    "print(p_opt_dyn_sched)\n",
    "print(f'Achieved profit is {margin_total_resp(p_opt_dyn_sched)}')\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"Price ($)\")\n",
    "plt.xticks(np.arange(T))\n",
    "plt.plot(np.arange(T), p_opt_dyn_sched, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple **temporal dependency** inside the price-demand function dictates a complex pricing strategy with price surges and discounts. It can be viewed as a formal justification of the **Hi-Lo pricing strategy** used by many retailers; we see how altering regular and promotional prices helps to maximize profit.\n",
    "\n",
    "The price-response function we have defined is essentially a differential equation where the profit depends not only on the current price action but also on the price dynamics. It's expected that such equations can exhibit very complex behavior, especially over long-time intervals, so the corresponding control policies can also become complex.\n",
    "\n",
    "Optimization of such policies requires powerful, yet flexible methods, such as **deep reinforcement learning.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Case Study 1: Pricing Policy Optimization using Deep Q Networks (DQN)**\n",
    "\n",
    "Although the greedy search algorithm we implemented produces the optimal pricing schedule for a simple differential price-response function, as we add more constraints and interdependencies, it becomes increasingly more <u>challenging to reduce the problem to standard formulations, such as linear or integer programming.</u>\n",
    "\n",
    "Let's approach the problem from a different perspective and apply a generic Deep Q Network (DQN) algorithm to learn the optimal price control policy.\n",
    "\n",
    "We implement the basic version of DQN from scratch to provide a clearer picture of how DQN is applied to this particular environment and to demonstrate several debugging techniques. \n",
    "\n",
    "## **Defining the environment**\n",
    "\n",
    "Reinforcement learning involves a setup where an agent interacts with the environment in discrete time steps with the goal of learning a reward-maximizing behavior policy. At each time step $t$, with a given state $s$, the agent takes an action $a$ according to its policy $\\pi(s) \\rightarrow a$ and receives the reward $r$ moving to the next state $s’$. \n",
    "\n",
    "Let's redefine our pricing environment in these reinforcement learning terms:\n",
    "\n",
    "1. The **state** $s$ of the environment at any time step $t$ is a vector of prices of all previous time steps concatenated with the one-hot encoding of the time steps itself.\n",
    "$s_t = \\left( p_{t-1}, p_{t-2}, \\ldots, p_{0} \\right)\\ |\\ \\left(0, \\ldots, 1, \\ldots, 0 \\right)$\n",
    "\n",
    "2. The **action** $a$ for every time step is just an index in the array of valid price levels. i.e. an array of shape (price_levels, time_steps)\n",
    "\n",
    "3. The **reward** $r$ is the margin.\n",
    "\n",
    "4. The goal is to find a policy that prescribes a pricing action based on the current state in a way that the total profit for a selling season (episode) is maximized.\n",
    "\n",
    "## **Overview of the DQN algorithm**\n",
    "\n",
    "The goal of the algorithm is to learn an action policy $\\pi$ that maximizes the total discounted cumulative reward $R$ (aka the return) earned during the episode of $T$ time steps:\n",
    "\n",
    "$R = \\sum_{t=0}^T \\gamma^t r_t$\n",
    "\n",
    "Such a policy can be defined if we know a function that estimates the expected return based on the current state and next action, under the assumption that all subsequent actions will also be taken according to the policy:\n",
    "\n",
    "$Q^{\\pi}(s,a) = \\mathbb{E}_{s,a}\\left[R\\right]$\n",
    "\n",
    "Assuming that this Q-function is known, the policy can be straightforwardly defined as follows to maximize the return:\n",
    "\n",
    "$\\pi(s) = \\underset{a}{\\text{argmax}}\\ Q(s,a)$\n",
    "\n",
    "We can combine the above definitions into the following recursive equation (the Bellman equation):\n",
    "\n",
    "$Q^{\\pi}(s,a) = r + \\gamma\\max_{a'} Q(s', a')$\n",
    "\n",
    "where $s'$ and $a'$ are the next state and next action taken in that state, respectively. If we estimate the Q-function using some approximator, then the quality of the approximation can be measured using the difference between the two sides of this equation:\n",
    "\n",
    "$\\delta = Q^{\\pi}(s,a) - \\left( r +\\gamma\\max_{a'} Q(s', a') \\right)$\n",
    "\n",
    "This is the **temporal difference error**. The main idea behind DQN is to train a deep neural network to approximate the Q-function using the temporal difference error as the loss function. \n",
    "\n",
    "In principle, the training process is quite straight forward:\n",
    "\n",
    "1. Initialize the network. Its input corresponds to state representation, while output is a vector of Q-valuesfor all actions.\n",
    "\n",
    "2. For each time step:\n",
    "    1. Estimate Q-values.\n",
    "    2. Execute the action with the maximum Q-value and observe the reward.\n",
    "    3. Calculate the temporal difference error, and use it to derive the loss function.\n",
    "    4. Update the network's parameters using stochastic gradient descent.\n",
    "    \n",
    " This simple approach is unstable for training complex non-linear approximators, such as deep neural networks. Two techniques are used to address the issue:\n",
    " \n",
    " 1. **Replay buffer**: One of the main problems with the basic training above is that the sequential observations are usually correlated, while network training generally requires independently distributed samples. DQN works around this by accumulating multiple observed transitions $(s, a, r, s')$ in a buffer and sampling batches of such transitions to retrain the network. The buffer is typically chosen large enough to minimize the correlations between samples.\n",
    " \n",
    " 2. **Target networks**: The second problem is that the network parameters are updated based on the loss function computed using the Q-values produced by the same network. In other words, the learning target moves simultaneously with the parameters we are trying to learn, making the optimization process unstable. DQN mitigates this issuse by maintaining two instances of the network. The first one is used to take actions and is continuously updated as described above. The second one, called a target network, is a **lagged copy** of the first one and used specifically to calculate the Q-values for the loss function (i.e. the target). This technique helps to stabilize the learning process.\n",
    " \n",
    " 3. **Action Selection Process**: The last concern that we need to address is how the action is chosen based on Q-values estimated by the network. A policy that always takes action with the maximum Q-value will not work well because the learning process will not sufficiently explore the environment, so we choose to randomize the action selection process. More specifically, we use $ε$-greedy policy that takes the action with the maximum Q-value with the probability of $1− ε$ and a random action with the probability of $ε$. We also use the annealing technique starting with a relatively large value of $ε$ and gradually decreasing it from one training episode to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Solution: Pricing Policy Optimization using DQN**\n",
    "\n",
    "A standalone DQN-based optimizer using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from itertools import count\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transition = namedtuple(\"transition\",\n",
    "                        (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "#a cyclic buffer of bounded size that holds the transitions observed recently\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class PolicyNetworkDQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(PolicyNetworkDQN, self).__init__()\n",
    "        layers = [\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q_values = self.model(x)\n",
    "        return q_values\n",
    "\n",
    "\n",
    "class AnnealedEpsGreedyPolicy(object):\n",
    "    def __init__(self, eps_start=0.9, eps_end=0.05, eps_decay=400):\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def select_action(self, q_values):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps_end + (\n",
    "            self.eps_start - self.eps_end) * math.exp(\n",
    "                -1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            return np.argmax(q_values)\n",
    "        else:\n",
    "            return random.randrange(len(q_values))\n",
    "\n",
    "\n",
    "GAMMA = 1.00\n",
    "TARGET_UPDATE = 20\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "\n",
    "def update_model(memory, policy_net, target_net):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(\n",
    "        map(lambda s: s is not None, batch.next_state)),\n",
    "                                  device=device,\n",
    "                                  dtype=torch.bool)\n",
    "    non_final_next_states = torch.stack(\n",
    "        [s for s in batch.next_state if s is not None]).cuda()\n",
    "\n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.stack(batch.reward).cuda()\n",
    "    state_action_values = policy_net(to_tensor(state_batch).cuda()).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device).cuda()\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(\n",
    "        1)[0].detach()\n",
    "\n",
    "    # compute the expected Q values\n",
    "    expected_state_action_values = reward_batch[:, 0] + (GAMMA *\n",
    "                                                         next_state_values)\n",
    "\n",
    "    # compute huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values,\n",
    "                            expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # model optimizer\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def env_initial_state():\n",
    "    return np.repeat(0, 2 * T)\n",
    "\n",
    "\n",
    "def env_step(t, state, action):\n",
    "    next_state = np.repeat(0, len(state))\n",
    "    next_state[0] = price_levels[action]\n",
    "    next_state[1:T] = state[:T - 1]\n",
    "    next_state[T + t] = 1\n",
    "    reward = margin_t_resp(next_state[0], next_state[1])\n",
    "    return next_state, reward\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    return torch.from_numpy(np.array(x).astype(np.float32))\n",
    "\n",
    "\n",
    "def to_tensor_long(x):\n",
    "    return torch.tensor([[x]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 of 1000 (0.00%)\n",
      "Episode 20 of 1000 (2.00%)\n",
      "Episode 40 of 1000 (4.00%)\n",
      "Episode 60 of 1000 (6.00%)\n",
      "Episode 80 of 1000 (8.00%)\n",
      "Episode 100 of 1000 (10.00%)\n",
      "Episode 120 of 1000 (12.00%)\n",
      "Episode 140 of 1000 (14.00%)\n",
      "Episode 160 of 1000 (16.00%)\n",
      "Episode 180 of 1000 (18.00%)\n",
      "Episode 200 of 1000 (20.00%)\n",
      "Episode 220 of 1000 (22.00%)\n",
      "Episode 240 of 1000 (24.00%)\n",
      "Episode 260 of 1000 (26.00%)\n",
      "Episode 280 of 1000 (28.00%)\n",
      "Episode 300 of 1000 (30.00%)\n",
      "Episode 320 of 1000 (32.00%)\n",
      "Episode 340 of 1000 (34.00%)\n",
      "Episode 360 of 1000 (36.00%)\n",
      "Episode 380 of 1000 (38.00%)\n",
      "Episode 400 of 1000 (40.00%)\n",
      "Episode 420 of 1000 (42.00%)\n",
      "Episode 440 of 1000 (44.00%)\n",
      "Episode 460 of 1000 (46.00%)\n",
      "Episode 480 of 1000 (48.00%)\n",
      "Episode 500 of 1000 (50.00%)\n",
      "Episode 520 of 1000 (52.00%)\n",
      "Episode 540 of 1000 (54.00%)\n",
      "Episode 560 of 1000 (56.00%)\n",
      "Episode 580 of 1000 (58.00%)\n",
      "Episode 600 of 1000 (60.00%)\n",
      "Episode 620 of 1000 (62.00%)\n",
      "Episode 640 of 1000 (64.00%)\n",
      "Episode 660 of 1000 (66.00%)\n",
      "Episode 680 of 1000 (68.00%)\n",
      "Episode 700 of 1000 (70.00%)\n",
      "Episode 720 of 1000 (72.00%)\n",
      "Episode 740 of 1000 (74.00%)\n",
      "Episode 760 of 1000 (76.00%)\n",
      "Episode 780 of 1000 (78.00%)\n",
      "Episode 800 of 1000 (80.00%)\n",
      "Episode 820 of 1000 (82.00%)\n",
      "Episode 840 of 1000 (84.00%)\n",
      "Episode 860 of 1000 (86.00%)\n",
      "Episode 880 of 1000 (88.00%)\n",
      "Episode 900 of 1000 (90.00%)\n",
      "Episode 920 of 1000 (92.00%)\n",
      "Episode 940 of 1000 (94.00%)\n",
      "Episode 960 of 1000 (96.00%)\n",
      "Episode 980 of 1000 (98.00%)\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "policy_net = PolicyNetworkDQN(2 * T, len(price_levels)).to(device)\n",
    "target_net = PolicyNetworkDQN(2 * T, len(price_levels)).to(device)\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=0.005)\n",
    "policy = AnnealedEpsGreedyPolicy()\n",
    "memory = ReplayBuffer(10000)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "num_episodes = 1000\n",
    "return_trace = []\n",
    "p_trace = []  # price schedules used in each episode\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env_initial_state()\n",
    "    reward_trace = []\n",
    "    p = []\n",
    "    for t in range(T):\n",
    "        # Select and perform an action\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(to_tensor(state).cuda())\n",
    "        action = policy.select_action(q_values.detach().cpu().numpy())\n",
    "\n",
    "        next_state, reward = env_step(t, state, action)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(to_tensor(state), to_tensor_long(action),\n",
    "                    to_tensor(next_state) if t != T - 1 else None,\n",
    "                    to_tensor([reward]))\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        update_model(memory, policy_net, target_net)\n",
    "\n",
    "        reward_trace.append(reward)\n",
    "        p.append(price_levels[action])\n",
    "\n",
    "    return_trace.append(sum(reward_trace))\n",
    "    p_trace.append(p)\n",
    "\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(\n",
    "            f'Episode {i_episode} of {num_episodes} ({i_episode/num_episodes*100:.2f}%)'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualizing and Debugging DQN Policy Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.palettes import PuBu4\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import Label\n",
    "output_notebook()\n",
    "\n",
    "def plot_return_trace(returns, smoothing_window=10, range_std=2):\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Return ($)\")\n",
    "    returns_df = pd.Series(returns)\n",
    "    ma = returns_df.rolling(window=smoothing_window).mean()\n",
    "    mstd = returns_df.rolling(window=smoothing_window).std()\n",
    "    plt.plot(ma, c = 'blue', alpha = 1.00, linewidth = 1)\n",
    "    plt.fill_between(mstd.index, ma-range_std*mstd, ma+range_std*mstd, color='blue', alpha=0.2)\n",
    "\n",
    "def plot_price_schedules(p_trace, sampling_ratio, last_highlights, fig_number=None):\n",
    "    plt.figure(fig_number);\n",
    "    plt.xlabel(\"Time step\");\n",
    "    plt.ylabel(\"Price ($)\");\n",
    "    plt.xticks(range(T))\n",
    "    plt.plot(range(T), np.array(p_trace[0:-1:sampling_ratio]).T, c = 'k', alpha = 0.05)\n",
    "    return plt.plot(range(T), np.array(p_trace[-(last_highlights+1):-1]).T, c = 'red', alpha = 0.5, linewidth=2)\n",
    "\n",
    "def bullet_graph(data, labels=None, bar_label=None, axis_label=None,\n",
    "                size=(5, 3), palette=None, bar_color=\"black\", label_color=\"gray\"):\n",
    "    stack_data = np.stack(data[:,2])\n",
    "\n",
    "    cum_stack_data = np.cumsum(stack_data, axis=1)\n",
    "    h = np.max(cum_stack_data) / 20\n",
    "\n",
    "    fig, axarr = plt.subplots(len(data), figsize=size, sharex=True)\n",
    "\n",
    "    for idx, item in enumerate(data):\n",
    "\n",
    "        if len(data) > 1:\n",
    "            ax = axarr[idx]\n",
    "\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_yticklabels([item[0]])\n",
    "        ax.set_yticks([1])\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "\n",
    "        prev_limit = 0\n",
    "        for idx2, lim in enumerate(cum_stack_data[idx]):\n",
    "            ax.barh([1], lim - prev_limit, left=prev_limit, height=h, color=palette[idx2])\n",
    "            prev_limit = lim\n",
    "        rects = ax.patches\n",
    "        ax.barh([1], item[1], height=(h / 3), color=bar_color)\n",
    "\n",
    "    if labels is not None:\n",
    "        for rect, label in zip(rects, labels):\n",
    "            height = rect.get_height()\n",
    "            ax.text(\n",
    "                rect.get_x() + rect.get_width() / 2,\n",
    "                -height * .4,\n",
    "                label,\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                color=label_color)\n",
    "            \n",
    "    if bar_label is not None:\n",
    "        rect = rects[0]\n",
    "        height = rect.get_height()\n",
    "        ax.text(\n",
    "            rect.get_x() + rect.get_width(),\n",
    "            -height * .1,\n",
    "            bar_label,\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            color='white')\n",
    "    if axis_label:\n",
    "        ax.set_xlabel(axis_label)\n",
    "    fig.subplots_adjust(hspace=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profit results: 3305066.0\n",
      "Best profit results: 3305630.0\n",
      "Best profit results: 3313748.0\n",
      "Best profit results: 3316173.0\n",
      "Best profit results: 3347341.0\n",
      "[[345, 55, 175, 120, 455, 130, 90, 255, 10, 345, 95, 390, 105, 380, 290, 175, 405, 135, 55, 20], [345, 260, 430, 165, 370, 385, 165, 225, 355, 160, 35, 490, 490, 415, 345, 400, 455, 470, 325, 235]] (1000, 20)\n",
      "[42527.0, 888845.0] (1000,)\n"
     ]
    }
   ],
   "source": [
    "plot_return_trace(return_trace)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "plot_price_schedules(p_trace, 10, 1, fig.number)\n",
    "\n",
    "for margin in sorted(margin_total_resp(s) for s in p_trace)[-5:]:\n",
    "    print(f'Best profit results: {margin}')\n",
    "    \n",
    "    \n",
    "print(p_trace[:2], np.shape(p_trace))\n",
    "print(return_trace[:2], np.shape(return_trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    }
   ],
   "source": [
    "plt.ioff()\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "def animate(t):\n",
    "    fig.clear()\n",
    "    plot_price_schedules(p_trace[0:t], 5, 1, fig.number)\n",
    "\n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=range(10, 1000, 10), interval=50, blit=False, repeat_delay=500)\n",
    "ani.save('sim.gif', dpi=80, writer='imagemagick', fps=20)\n",
    "rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal price action 495\n",
      "<Figure size 1600x500 with 1 Axes>\n",
      "<Figure size 1600x500 with 1 Axes>\n",
      "<Figure size 1600x500 with 1 Axes>\n",
      "<Figure size 1600x500 with 1 Axes>\n"
     ]
    }
   ],
   "source": [
    "# Visualize Q values for a given state\n",
    "sample_state = [170.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., \\\n",
    "                1.,     0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]\n",
    "\n",
    "Q_s = policy_net(to_tensor(sample_state).cuda())\n",
    "a_opt = Q_s.max(0)[1].detach()\n",
    "print(f\"Optimal price action {price_levels[a_opt]}\")\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.xlabel(\"Price action ($)\")\n",
    "plt.ylabel(\"Q ($)\")\n",
    "plt.bar(price_levels, Q_s.cpu().detach().numpy(), color='crimson',  width=6, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20, 2) (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Playing several episods and recording Q-values with the corresponding actual retunrs\n",
    "\n",
    "num_episodes = 100\n",
    "return_trace = []\n",
    "q_values_rewards_trace = np.zeros((\n",
    "    num_episodes,\n",
    "    T,\n",
    "    2,\n",
    "))\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env_initial_state()\n",
    "    for t in range(T):\n",
    "        #select and perform an action\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(to_tensor(state).cuda()).detach().cpu().numpy()\n",
    "            action = policy.select_action(q_values)\n",
    "            next_state, reward = env_step(t, state, action)\n",
    "\n",
    "            #move to the next state\n",
    "            state = next_state\n",
    "            q_values_rewards_trace[i_episode][t][0] = q_values[action]\n",
    "            for tau in range(t):\n",
    "                q_values_rewards_trace[i_episode][tau][1] += reward**(GAMMA**(\n",
    "                    t - tau))\n",
    "\n",
    "q_values = np.reshape(q_values_rewards_trace, (\n",
    "    num_episodes * T,\n",
    "    2,\n",
    "))\n",
    "\n",
    "print(np.shape(q_values_rewards_trace), np.shape(q_values))\n",
    "\n",
    "df = pd.DataFrame(data=q_values, columns=['Q-value', 'Return'])\n",
    "g = sns.jointplot(x=\"Q-value\",\n",
    "                  y=\"Return\",\n",
    "                  data=df,\n",
    "                  kind=\"reg\",\n",
    "                  color=\"crimson\",\n",
    "                  height=10)\n",
    "g.plot_joint(plt.scatter, c=\"k\", s=30, linewidth=1, marker=\"+\", alpha=0.4)\n",
    "g.ax_joint.collections[0].set_alpha(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Solution: Implementing DQN Optimizer using RLLib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-10 19:07:30,399\tINFO services.py:1265 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:5000\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497115.94\n",
      "671873.5\n",
      "952896.56\n",
      "1100033.0\n",
      "1237012.0\n",
      "1352408.5\n",
      "1441823.8\n",
      "1492972.2\n",
      "1536437.4\n",
      "1588033.9\n",
      "1633372.9\n",
      "1674680.6\n",
      "1690634.2\n",
      "1713502.2\n",
      "1740390.1\n",
      "1758521.5\n",
      "1771984.5\n",
      "1795958.1\n",
      "1805755.4\n",
      "1817736.0\n",
      "1819310.9\n",
      "1866289.6\n",
      "2127746.0\n",
      "2350825.2\n",
      "2456490.5\n",
      "2519889.8\n",
      "2565558.8\n",
      "2598124.0\n",
      "2627785.8\n",
      "2658396.2\n",
      "2659941.5\n",
      "2669322.0\n",
      "2668884.0\n",
      "2662253.2\n",
      "2656018.0\n",
      "2648351.5\n",
      "2637730.5\n",
      "2622466.0\n",
      "2600911.5\n",
      "2592401.5\n",
      "2587358.0\n",
      "2578432.2\n",
      "2563792.2\n",
      "2558352.5\n",
      "2550244.5\n",
      "2544414.2\n",
      "2535565.5\n",
      "2535701.2\n",
      "2538289.5\n",
      "2552169.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import ray\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "class PricingEnv(Env):\n",
    "    def __init__(self, config):\n",
    "        self.reset()\n",
    "        self.action_space = Discrete(len(price_levels))\n",
    "        self.observation_space = Box(0,\n",
    "                                     10000,\n",
    "                                     shape=(2 * T, ),\n",
    "                                     dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = env_initial_state()\n",
    "        self.t = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state, reward = env_step(self.t, state, action)\n",
    "        self.t += 1\n",
    "        self.state = next_state\n",
    "        return next_state, reward, self.t == T - 1, {}\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(dashboard_port=5000, local_mode=True, include_dashboard=True)\n",
    "\n",
    "\n",
    "def train_dqn():\n",
    "    config = dqn.DEFAULT_CONFIG.copy()\n",
    "    config[\"log_level\"] = \"WARN\"\n",
    "    config[\"lr\"] = 0.0001\n",
    "    config[\"gamma\"] = 0.95\n",
    "    config[\"train_batch_size\"] = 64\n",
    "    config[\"buffer_size\"] = 100000\n",
    "    config[\"timesteps_per_iteration\"] = 5000\n",
    "    config[\"hiddens\"] = [256, 256, 128]\n",
    "    config[\"exploration_config\"] = {\n",
    "        'type': 'EpsilonGreedy',\n",
    "        'initial_epsilon': 1.0,\n",
    "        'final_epsilon': 0.02,\n",
    "        'epsilon_timesteps': 10000\n",
    "    }\n",
    "    trainer = dqn.DQNTrainer(config=config, env=PricingEnv)\n",
    "    error = []\n",
    "    for i in range(50):\n",
    "        result = trainer.train()\n",
    "        error.append(result[\"info\"][\"learner\"][\"default_policy\"]\n",
    "                     [\"learner_stats\"][\"mean_td_error\"])\n",
    "        print(np.mean(error))\n",
    "\n",
    "train_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-pricing",
   "language": "python",
   "name": "rl-pricing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
